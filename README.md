# 21F3002170_IITMBS_MLOPS_OPPE2

Submission for **OPPE2 (MLOps)** ‚Äì IITM BS Program.  
This project demonstrates an **end-to-end production-ready deployment** for a **Heart Disease Prediction** system with explainability, fairness, observability, scalability, performance monitoring, drift detection, and robustness testing.

---

## üìÅ Repository Structure

- `HeartDiseaseTrainingAndPrediction.ipynb` ‚Äì Extended training notebook with all tasks  
- `data.csv` ‚Äì Provided training dataset  
- `data/generated/` ‚Äì Synthetic datasets for testing and monitoring  
- `models/` ‚Äì Trained models, SHAP outputs, drift reports  
- `app/` ‚Äì FastAPI application code (API server)  
- `k8s/` ‚Äì Kubernetes manifests (Deployment, Service, HPA)  
- `perf/` ‚Äì Load-testing scripts (`post.lua`) and wrk results  
- `logs/` ‚Äì Logs of predictions, monitoring, drift, poisoning experiments  
- `Dockerfile`, `requirements.txt` ‚Äì Containerization setup  
- `README.md` ‚Äì This file  

---

## 1Ô∏è‚É£ GitHub Setup

- Created a private repo: **21F3002170_IITMBS_MLOPS_OPPE2**  
- Added IITMBSMLOps as collaborator  
- Configured SSH authentication from GCP VM  
- Structured commits after each integration stage with meaningful messages  

---

## 2Ô∏è‚É£ Explainability (10 marks)

We used **SHAP** to interpret the Logistic Regression model.

- **Global importance**: `oldpeak`, `thalach`, `trestbps`, `chol`, and `age` strongly influence predictions.  
- **Local explanations**: For positive predictions, SHAP identified top 5 contributing features per patient.  

‚úÖ Outputs:  
- `models/shap_summary.png` ‚Äì beeswarm summary  
- `models/shap_bar_top20.png` ‚Äì bar plot of feature importance  
- `models/local_explanations.json` ‚Äì per-sample explanations  

---

## 3Ô∏è‚É£ Fairness (10 marks)

Using **Fairlearn**, with **gender (sex)** as sensitive attribute:

- Compared accuracy, precision, recall, selection rate, ROC-AUC by group  
- Observed disparities:  
  - **Demographic Parity Difference:** 0.214  
  - **Equalized Odds Difference:** 0.182  

‚ö†Ô∏è Indicates moderate fairness concerns between male and female predictions.  

---

## 4Ô∏è‚É£ Dockerized API on GCP (30 marks)

- Developed **FastAPI** service with:
  - `/predict` ‚Äì returns prediction + probability  
  - `/healthz` ‚Äì health check  
  - `/metrics` ‚Äì Prometheus metrics  

- Containerized with **Docker** (`heart-api:local`)  
- Deployed on **Google Kubernetes Engine (GKE)**:  
  - Deployment with resource requests/limits  
  - Service as LoadBalancer  
  - HPA (Horizontal Pod Autoscaler) with min=1, max=3 pods  

‚úÖ Verified predictions through external IP and confirmed autoscaling.  

---

## 5Ô∏è‚É£ Logging & Observability (20 marks)

- Generated **100 synthetic rows** and sent them to the API.  
- Logged per-sample:
  - Inputs, prediction, probability, latency, timestamp  
- Saved logs in both JSONL and CSV formats.  

### Observability
- Exposed Prometheus metrics at `/metrics`:  
  - `predictions_total`  
  - `prediction_latency_seconds`  
- Verified counters and histograms before/after sending requests.  

---

## 6Ô∏è‚É£ Performance Monitoring & Timeout Analysis (10 marks)

Used **wrk** load testing with rotating payloads.

### Key Runs
- **c=32, t=4, 15s** ‚Üí ~180 req/s baseline  
- **c=200, t=8, 60s** ‚Üí 554 req/s, p50=324 ms, p99=1.02 s, 2 timeouts  
- **c=300, t=8, 60s, timeout=1s** ‚Üí multiple client timeouts  

### Autoscaling
- HPA scaled pods from 1 ‚Üí 3 under high load  
- Verified with `kubectl get hpa -w` and `kubectl get pods -w`  

‚úÖ Logs in `perf/` directory with wrk outputs and screenshots.  

---

## 7Ô∏è‚É£ Input Drift Detection (10 marks)

### Method 1 ‚Äì Statistical Tests
- **KS test** for continuous, **Chi-square** for categorical  
- Significant drift found in `oldpeak`, `cp`, `sex`  

### Method 2 ‚Äì Drift Classifier
- Trained binary classifier (train=0, new=1)  
- **CV AUC = 0.73, Holdout AUC = 0.79** ‚Üí moderate drift  
- Drift driven by: `oldpeak`, `thalach`, `trestbps`, `chol`, `age`, `cp`, `sex`  

‚úÖ Outputs in `logs/` with ROC curve, feature importances, and report JSON.  

---

## 8Ô∏è‚É£ Data Poisoning Attack (10 marks)

Simulated **20% label flipping** (`yes ‚Üî no`) in training data.  

| Model      | Accuracy | Precision | Recall | AUC |
|------------|----------|-----------|--------|-----|
| Clean      | 0.84     | 0.86      | 0.82   | 0.90 |
| Poisoned   | 0.72     | 0.69      | 0.71   | 0.76 |

- Clear degradation across all metrics.  
- Plotted side-by-side bar chart (`logs/poisoning_comparison.png`).  

---

## ‚úÖ Conclusion

This project demonstrates a **full MLOps workflow**:
- **Explainability** (SHAP)  
- **Fairness** (Fairlearn)  
- **Deployment** (Docker, FastAPI, GKE, HPA)  
- **Logging & Observability** (structured logs + Prometheus metrics)  
- **Performance Monitoring** (wrk throughput, latency, timeouts, scaling)  
- **Input Drift Detection** (statistical + model-based)  
- **Adversarial Robustness** (label-flip poisoning)  

This satisfies all rubric items and shows how to build an **explainable, fair, scalable, observable, and robust ML system in production**.

---
